datasets:
  # class_name, data_name & data_sampling_ratio are not used but need to be passed to avoid errors
  - class_name: MegatronDataset
    data_name: Megatron
    data_sampling_ratio: 1
    class_args:
      eval_steps: 2
      data_cache_path: cache
      # Option 1: data loading using --data-path with single file
      data_path:
        - 1
        - /proj/datasets/moduleformer/dataset=openwebmath
        - 41
        - /proj/datasets/moduleformer/refinedweb_starcoder_tokens_megatron
        - 1
        - /proj/datasets/moduleformer/dataset=peS2o-v2-1-en
        - 20
        - /proj/datasets/moduleformer/starcoderdata_starcoder_tokens_megatron
        - 1
        - /proj/datasets/moduleformer/dataset=pile-arxiv-1-en
        - 0.812
        - /proj/datasets/moduleformer/dataset=pile-books3-1-en
        - 0.463
        - /proj/datasets/moduleformer/dataset=pile-freelaw-1-en
        - 0.86
        - /proj/datasets/moduleformer/dataset=pile-openwebtext2-1-en
        - 1.388
        - /proj/datasets/moduleformer/dataset=pile-pubmed-central-1-en
        - 0.048
        - /proj/datasets/moduleformer/dataset=pile-bookcorpus2-1-en
        - 0.243
        - /proj/datasets/moduleformer/dataset=pile-dm-mathematics-1-en
        - 0.015
        - /proj/datasets/moduleformer/dataset=pile-enron-emails-1-en
        - 0.083
        - /proj/datasets/moduleformer/dataset=pile-europarl-1-en
        - 0.243
        - /proj/datasets/moduleformer/dataset=pile-gutenberg-pg-19-1-en
        - 0.058
        - /proj/datasets/moduleformer/dataset=pile-hackernews-1-en
        - 0.023
        - /proj/datasets/moduleformer/dataset=pile-nih-exporter-1-en
        - 0.120
        - /proj/datasets/moduleformer/dataset=pile-opensubtitles-1-en
        - 0.035
        - /proj/datasets/moduleformer/dataset=pile-philpapers-1-en
        - 0.257
        - /proj/datasets/moduleformer/dataset=pile-pubmed-abstracts-1-en
        - 0.285
        - /proj/datasets/moduleformer/dataset=pile-uspto-backgrounds-1-en
        - 0.111
        - /proj/datasets/moduleformer/dataset=pile-ubuntu-irc-1-en
        - 0.076
        - /proj/datasets/moduleformer/dataset=pile-youtubesubtitles-1-en
        - 6.7
        - /proj/datasets/moduleformer/dataset=pile-wikipedia-en-1-en
        - 4.8
        - /proj/datasets/moduleformer/dataset=pile-stackexchange-1-en
        - 0.878
        - /proj/datasets/moduleformer/StackMathQA-train_text_document
        - 0.121
        - /proj/datasets/moduleformer/TemplateGSM-train_text_document
        - 0.103
        - /proj/datasets/moduleformer/Magicoder-Evol-Instruct-110K-train_text_document
        - 0.064
        - /proj/datasets/moduleformer/Magicoder-OSS-Instruct-75K-train-rft_text_document
        - 0.217
        - /proj/datasets/moduleformer/Code-290k-ShareGPT-train_text_document
        - 0.512
        - /proj/datasets/moduleformer/commitpackft_train_text_document
        - 0.103
        - /proj/datasets/moduleformer/evol-codealpaca-v-train_text_document
        - 0.601
        - /proj/datasets/moduleformer/OpenHermes-2.5-train_text_document
        - 0.393
        - /proj/datasets/moduleformer/ultrachat-train_text_document
        - 0.005
        - /proj/datasets/moduleformer/oasst_octopack_train_text_document
        - 4.9
        - /proj/datasets/moduleformer/dataset=UltraTextbooks-2.0-1-en
        - 4.8
        - /proj/datasets/moduleformer/xp3x_nl_5lang_merged_train_text_document
        - 4.8
        - /proj/datasets/moduleformer/dataset=algebraic-stack-1-en
      split: 100,0,0
      sequence_length: 4096

tokenizer_args:
  tokenizer_name: bigcode/starcoder

model_args:
  model_class: AutoModelForCausalLM
  pretrained_config:
    activation_function: swiglu
    add_bias: false
    attention_softmax_in_fp32: true
    attn_pdrop: 0
    embd_pdrop: 0
    resid_pdrop: 0
    initializer_range: 0.02
    layer_norm_epsilon: 1e-05
    model_type: gpt_megatron
    n_embd: 2560
    n_head: 32
    n_inner: 8192
    n_layer: 32
    n_positions: 4096
    normalization_function: rmsnorm
    position_embedding_type: rope
    rope_theta: 10000
    scale_attention_softmax_in_fp32: true
    attention_head_type: mha
    scale_attn_weights: true
    vocab_size: 50304
    m_width: 10
    init_method: mup
    tie_word_embeddings: false
  efficient_cpu_initialization: false
  attention_implementation: flash_attention_2
  use_padding_free_transformer: true

tuning_args:
  tuning_method: pretraining

save_args:
  save_path: /proj/checkpoints/mayank/dolomite-exp1-p2
  save_interval: 5000

load_args:
  load_path: /proj/checkpoints/mayank/dolomite-exp1
  load_lr_scheduler: false
  load_dataloader_state: false
  load_experiments_tracker_state: false
  load_starting_iteration: false

logging_args:
  log_interval: 10
  experiments_tracker_name: wandb
  wandb_args:
    project: granite-3b-experiments
    name: dolomite-exp1

training_parameters:
  num_training_steps: 62500
  eval_interval: 2500000
  micro_batch_size: 2
  gradient_accumulation_steps: 2
  eval_during_training: false

optimizer_args:
  params_group_method: mup
  class_name: TorchAdamW
  class_args:
    lr: 0.0035
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95
    eps: 1e-10

lr_scheduler_args:
  lr_decay_style: exponential
  num_warmup_steps: 0
  num_constant_steps: 0
  num_decay_steps: 62500

mixed_precision_args:
  dtype: bf16

distributed_args:
  distributed_backend: torch
  stage: 3
  zero_hpz_partition_size: 8
